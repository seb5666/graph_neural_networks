\documentclass[12pt]{article}

\usepackage{amsthm}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{graphicx}
\graphicspath{ {images/}{figs/}{figs/node_svm/} }
\usepackage{listings}
\lstset{
  basicstyle=\ttfamily,
  columns=fullflexible,
  frame=single,
  breaklines=true,
}

\theoremstyle{definition}
\newtheorem{definition}{Definition}

\title{{\small{Machine Learning and Algorithms for Data Mining} \\
Assessment 2\footnote{Word count: 2467 --- Computed using TexCount}} \\
\textit{Analysis of graph-structured data}}
\author{Sebastian Borgeaud --- spb61@cam.ac.uk}

\begin{document}
\maketitle

\begin{abstract}
	In this report I explore the efficiency of various algorithms on the task of transductive classification on a graph. In particular, I focus on the cora dataset, consisting of 2078 scientific publications each classified into one of seven classes. Each publication is described by a bag-of-word vector for a vocabulary of 1433 unique words. The edges in the graph represent citations (REFERENCE).
	\end{abstract}

\section{Main contributions, Key Concepts and Ideas}

TODO: Background / Related work

\subsection{Introduction}
Many real world datasets occur naturally in the form of graphs, for example protein-protein interaction networks in biology, social networks in social sciences and relational databases to name just a few examples from different fields. (CITATIONS) In this report I focus on the problem of node classification in the transductive setting. At training time, the entire structure of the network is known but only few nodes are labelled. At test time, we wish to infer the labels of some or all of the remaining nodes. This differs from supervised learning in two ways: i) the data points (the nodes) are connected to each other and ii) the features of the nodes that will be classified at test time are known in advance, i.e. the graph is known at test time. 
Instead, the problem can be seen as a graph-based semi-supervised learning problem, where the label information is smoothed over the graph via some form of explicit graph-based regularisation. For example, we could make the assumption that adjacent nodes are more likely to have the same class label and incorporate this in the loss
\begin{equation}
	\mathcal{L} = \mathcal{L}_0 + \lambda \mathcal{L}_{\mathit{reg}}
\end{equation}
where $\mathcal{L}_0$ is some supervised loss w.r.t to labeled nodes and $\mathcal{L}_{\mathit{reg}}$ is a graph Laplacian regularisation term using the made assumption.

\subsection{Cora Dataset}
I focus on the cora (CITATION) dataset, consisting of 2078 machine learning publications, each representing a node in the graph (REFERENCE). Each publication is classified into one of seven classes: `Case Based', `Genetic Algorithms', `Neural Networks', `Probabilistic Methods', `Reinforcement Learning', `Rule Learning' or `Theory'. Each publication has a binary bag-of-word feature vector for a vocabulary of 1433 unique words. The edges are directed (WRONG, adj.transpose == adj) and represent citations, where an edge $\mathit{pub_1} \to \mathit{pub_2}$ means that publication $\mathit{pub_2}$ is cited in publication $\mathit{pub_1}$. The papers were selected in such a way that every paper cites or is cited by at least one other paper.

In particular, I use the data split introduced by Kipf and Welling \cite{kipf2017semi}. At training time the label of only 140 nodes is given (about 6.7\%). A further 500 node labels are given as validation data. We wish to infer the label of 1000 nodes not contained in either the test or validation set. Furthermore, the edge orientations are ignored by constructing a symmetric adjacency matrix.

\subsubsection{Node degree distribution}
The distribution of node degrees is plotted in figures \ref{fig/node_degrees} and \ref{fig/node_degrees_truncated}. Most nodes have few outgoing edges, with 59.9\% of the nodes having 3 outgoing edges or less and over 96.5\% having 10 or fewer. There seems to be one extreme outlier publication making 168 citations, whereas the second most citing paper makes only 78 citations.

\begin{figure}[h]
	\includegraphics[width=1.0\textwidth]{node_degrees}
	\centering
	\caption{Distribution of node degrees in Cora dataset}
	\label{fig/node_degrees}
\end{figure}

\begin{figure}[h]
	\includegraphics[width=1.0\textwidth]{node_degrees_truncated}
	\centering
	\caption{Distribution of node degrees for nodes with degree below 40. This includes 2701 of the 2708 publications, or above 99.7\% of the nodes.}
	\label{fig/node_degrees_truncated}
\end{figure}

\subsubsection{Clusters}

\section{Methods}
\subsection{SVM on features alone}
First, I create a simple baseline model by training an SVM on the features of the nodes only, i.e.\ the bag-of-words vectors of the publications. This approach ignores the structure of the graph entirely and is therefore an instance of a supervised learning problem. In particular, given a bag-of-word representation of a publication we wish to infer its publication type. 

I trained the SVM on the training instances of the dataset. The trained model is then used to predict the class labels of the test publications. The model hyper-parameters are optimised using the validation data set. For the kernel I considered both a linear and a radial basis function.

\subsection{SVM with neighbour features}
\label{section/SVM_neighbour_features}
Next, I incorporate some information about the graph structure into the SVM model. To do this I combine the features of the neighbours of a node by taking their average and concatenating the resulting feature vector to the feature vector of the node itself. That is, we train the SVM on the features $x'$ where for each node $i$,
\[
	x'^{(i)} = \big[x^{(i)} || x_{neighbours}^{(i)} \big]
\]
where
\[
	x_{neighbours}^{(i)} = \frac{1}{\left\vert \mathcal{N}(i) \right\vert} \sum_{j \in N(i)} x^{(j)}
\]
where $N(i)$ is the set of neighbour nodes of node $i$.

\subsection{Feedforward neural network with neighbour features}
Next, I train a simple feedforward neural network on the simple concatenation of the node features with the average of the features of the neigbouring nodes. The network consists of two fully-connected layers. The first layer consists of 64 hidden units and has a ReLU activation function. The second layer consists of 7 output units, to which is applied a softmax activation to obtain a probability distribution over the 7 classes. Furthermore, as the number of training data is very small, I make use of heavy regularisation using Dropout \cite{srivastava2014dropout} on both the input layer and on the output of the first hidden layer. I also add an L2 regularisation cost on the weights of the first hidden layer.

\subsection{Graph Convolutional Network}
The third algorithm I tested on this dataset is the Graph Convolutional Network, presented by Kipf and Welling \cite{kipf2017semi}. The graph structure is encoded directly using a neural network $f(X, A)$ where $X$ is an $\textrm{N} \times \textrm{F}$ matrix containing in each row $i$ the features for node $i$ and $A$ is the $\textrm{N} \times \textrm{N}$ adjacency matrix. The model is trained on a supervised loss $\mathcal{L}_0$. This allows the model to distribute the gradient information from $\mathcal{L}_0$ to all nodes and therefore learn representations for nodes with and without labels \cite{kipf2017semi}.

\bigskip

A Graph Convolutional Network (GCN) is a neural network with the following layer-wise propagation rule, where the activations in each layer $H^{(l)}$ are computed as
\[
H^{(l+1)} = \sigma \big( \tilde{D}^{-\frac{1}{2}} \tilde{A} \tilde{D}^{-\frac{1}{2}} H^{(l)} W^{(l)}\big)
\]
Here, $\tilde{A} = A + I_n$ is the adjacency matrix with self-connections, hence adding the identity matrix $I_n$. $\tilde{D}$ is the diagonal degree matrix of $\tilde{A}$, that is $\tilde{D}_{ii} = \sum_{j} \tilde{A}_{ij}$. $W^{(l)}$ are the trainable neural network weights in layer $l$. Finally, $\sigma(\cdot)$ is an activation function.

\bigskip

For the cora dataset, the authors present a 2 layer GCN with a ReLU activation in the first layer and a softmax activation in the second layer. The computation of the forward pass of the neural network can therefore be described concisely as
\[
f(X,A) = \textrm{softmax}\big(\hat{A}\ \textrm{ReLU}\big( \hat{A} X W^{(0)} \big)\ W^{(1)} \big)
\]
where $\hat{A} = \tilde{D}^{-\frac{1}{2}} \tilde{A} \tilde{D}^{-\frac{1}{2}}$ and can be precomputed in a pre-processing step. $W^{(0)}$ is an $\textrm{N} \times \textrm{H}$ weight matrix of reals, where H is the number of nodes in the hidden layer and $W^{(1)}$ is an $\textrm{H} \times \textrm{C}$ weight matrix, where C is the number of classes, i.e.\ 7  for the Cora dataset. The ReLU activation function just clips the input to 0,
\[
\textrm{ReLU}(x) = \max(0, x)
\]
and the softmax activation computes for each component $x_i$
\[
\textrm{softmax}(x_i) = \frac{\exp(x_i)}{\sum_j \exp(x_j)}
\]
During training the cross-entropy loss 
\[
\mathcal{L} = - \sum_{l \in \textrm{y}_L} \sum_{c=1}^{C} Y_{lc} \ln f(X,A)_{lc}
\]
is minimised over all labeled examples $\textbf{y}_L$ using gradient descent. As the entire dataset fits in memory, we can use a single batch containing the entire network. Furthermore, dropout \cite{srivastava2014dropout} is added to the input features and to the output of the first layer.

\subsection{Graph Attention Networks}
Next, I implemented the Graph Attention Networks (GAT) algorithm presented by Veli{\v{c}}kovi{\'{c}} et al.\ \cite{velickovic2018graph}. Attention mechanism have become widely popular and have achieved state-of-the-art results in sequence-based tasks, see for example Bahdanau et al.\ (2014) \cite{bahdanau2014neural} or Vaswani et al.\ (2017) \cite{vaswani2017attention} as they have the advantage of giving the neural network the ability to learn on which part of the input to focus. More specifically, in the GAT case we talk about self-attention because the attention mechanism is used to compute a representation of a single sequence. The main idea behind the algorithm is to compute a hidden representation (also called an \textbf{embedding}) for each node in the graph by attending over all its neighbours using self-attention. That is, the network computes an embedding for each node using the nodes in its neighbourhood and by choosing how much attention to pay to each individual neighbour using the self-attention mechanism.

\bigskip

The GAT architecture is best described in terms of its layers, called \textbf{graph attention layers}. In each layer, the input features $\textbf{X}$ are first linearly transformed into $\textbf{X}' = \textbf{X}\textbf{W}$ where $W$ is a $\textrm{F} \times \textrm{F}'$ weight matrix. Thus each node feature vector $h_i = X_i$ is linearly transformed to a feature vector $h'_i = X'_i = h_i \textbf{W}$ of length $F'$. Then, self-attention is performed on the nodes by applying a shared attentional mechanism defined as a function $a: \mathbb{R}^{F'} \times \mathbb{R}^{F'} \to \mathbb{R}$. This function maps two transformed input feature vectors to a real attention coefficient:
\[
e_{ij} = a(h'_i, h'_j) = a(h_i \textbf{W}, h_j \textbf{W})
\]

Intuitively, $e_{ij}$ indicates how important the features of node $j$ are to node $i$. To inject graph structural information, we only attend over the some neighbourhood $\mathcal{N}(i)$ for a given node $i$, defined to be the direct neighbours of $i$, including $i$ itself. Hence, we have that
\begin{equation*}
e_{ij} = \begin{cases}
				a(h_i \textbf{W}, h_j \textbf{W}) &\text{if $A_{ij} = 1$}\\
				0 &\text{otherwise}
			\end{cases}
\end{equation*}
Then, the coefficients are normalised using the softmax function:
\[
\alpha_{ij} = \textrm{softmax}(e_{ij}) = \frac{\exp(e_{ij})}{\sum_{k \in \mathcal{N}(i)} \exp(e_{ik})}
\]
We can do this for every pair of nodes $i$ and $j$ to obtain an $\textrm{N} \times \textrm{N}$ matrix $\alpha_{ij}$ of attention coefficients.

Here, the attention mechanism is defined to be a single-layer neural network, parametrised by weights $\textbf{a}$ and using a LeakyReLU activation: 
\[
\textrm{LeakyReLU}_{\alpha}(x) = 
	\begin{cases}
		x &\text{if $x \ge 0$}\\
		\alpha x &\text{otherwise}
	\end{cases}
\]
The attention coefficients can therefore be described concisely as
\[
\alpha_{ij} = \frac{\exp \big( \textrm{LeakyReLU}\big( \textbf{a}^T \big[ h_i \textbf{W} \vert\vert h_j \textbf{W} \big] \big) \big)}
{
\sum_{k \in \mathcal{N}(i)} \exp \big( \textrm{LeakyReLU}\big( \textbf{a}^T \big[ h_i \textbf{W} \vert\vert h_k \textbf{W} \big] \big) \big)
}
\]
Next, we use the attention coefficients to compute a linear combination of the transformed features and then applying a nonlinearity. This gives the output of a graph attention layer
\[
g_i = \sigma \big( \sum_{j \in \mathcal{N}(i)} \alpha_{ij} h_j \textbf{W} \big)
\]
Furthermore, we can apply multi-head attention to stabilise the learning process. Instead of using a single attention mechanism, we use $K_a$ simultaneously but independently, concatenating their output:
\[
g_i = {\Big\vert\Big\vert}_{k=1}^{K_a} \sigma \big( \sum_{j \in \mathcal{N}(i)} \alpha_{ij}^k h_j \textbf{W}^k \big)
\]
Note that in this case the output feature vectors will have size $K_a F'$.

\bigskip

Veli{\v{c}}kovi{\'{c}} et al.\ propose a GAT network consisting of two graph attention layers. The first layer has $K_a=8$ parallel attention heads and computes a hidden representation of features of size $F'=8$. The output features, or hidden representation computed by the first layer therefore have size $64$. The proposed nonlinearity is an exponential linear unit ELU:
\[
\textrm{ELU}_{\alpha}(x) = 
	\begin{cases}
		x &\text{if $x \ge 0$}\\
		\alpha (\exp(x) - 1) &\text{otherwise}
	\end{cases}
\]
The second layer has $C$ features and a single activation head $K_a=1$. The nonlinearity used is the standard softmax activation, which allows us to interpret the outputs as class probabilities.

Again, to cope with the small amount of training data, L2 regularisation is applied to the layer weights and Dropout \cite{srivastava2014dropout} is applied to the layers' inputs and to the attention coefficients. Thus at each training epoch, a node is only exposed to a sample of its neighbourhood. 

\section{Results \& analysis}

\begin{table}[h]
\centering	
\begin{tabular}{ |c c|c c|c|c|c| } 
 \hline
 \multicolumn{2}{|c|}{SVM on node features} & \multicolumn{2}{|c|}{SVM on averaged features} & features average MLP & GCN & GAT\\
 linear kernel & rbf kernel & linear kernel & rbf kernel & linear kernel & &\\
 \hline
 \hline
 0.558 &  0.566 & 0.714 & 0.716 & 0.762 & 0.785 & 0.769\\ 
 0.583 &  0.576 & 0.728 & 0.733 & 0.769 & 0.802 & 0.994\\ 
 \hline
\end{tabular}
\caption{Validation accuracy and test accuracy obtained on the different models.}
\end{table}

\subsection{SVM on node features}
The dataset includes 500 validation nodes. Using these we can easily optimise the hyper-parameters of the SVM by using grid-search over a range of parameters. I consider both a linear kernel and a radial basis function (RBF) kernel. For the linear kernel, the only hyper-parameter, $C$, is taken at regular intervals on a logarithmic scale ranging from $2^{-15}$ to $2^3$. For the RBF kernel, $C$ is similarly taken in the range $2^{-15}$ to $2^5$ and the second hyper-parameter, $\gamma$, is taken from the range $2^{-15}$ to $2^3$. The validation accuracy obtained for these hyper-parameters ranges are presented in figures \ref{fig/node_svm_linear_reg} and \ref{fig/node_svm_rbf_reg}. Using the hyper-parameters giving the highest validation accuracy, I obtain a test accuracy with a value of 56.6\%. 

Although well below the current state-of-the-art accuracy, this provides a simple baseline accuracy on the test nodes of 57.6\%. This is significantly better than a random baseline, which would have an expected accuracy of $\frac{1}{7}$ or about 14.3\%. This suggests that, rather unsurprisingly, a lot of the information required for classification is contained in the nodes features, that is in the bag-of-word vectors. 

\begin{figure}[h]
	\includegraphics[width=1.0\textwidth]{node_svm/linear_reg}
	\centering
	\caption{Validation accuracy obtained for various values of $C$ for the SVM with a linear kernel trained on the node features alone. The best validation accuracy is obtain at $C=0.024$, with a validation accuracy of 55.8\%.}
	\label{fig/node_svm_linear_reg}
\end{figure}
\begin{figure}[h]
	\includegraphics[width=1.0\textwidth]{node_svm/rbf_reg}
	\centering
	\caption{Validation accuracy obtained for various values of $C$ and $\gamma$ for the SVM with an RBF kernel trained on the node features alone. The best validation accuracy is obtain at $C=1.86$ and $\gamma=2^{-7}$, with a validation accuracy of 55.6\%.}
	\label{fig/node_svm_rbf_reg}
\end{figure}

\subsection{Average features SVM}
A simple way of incorporating knowledge about the graph structure is to represent each node by its features concatenated with the average of the features of its neighbouring nodes, as presented in section \ref{section/SVM_neighbour_features}. Using these new features I train a SVM as before, again using the validation nodes to select the hyper-parameters. Again I consider both a linear kernel and a RBF kernel. The validation accuracies are presented in figures \ref{fig/average_svm_linear_reg} and \ref{fig/average_svm_rbf_reg}. Again, the RBF kernel performs slightly better with a validation accuracy of 71.6\% compared to 71.4\% for the linear kernel. Using the best hyper-parameters with the RBF kernel I obtain an accuracy of 73.3\% on the test nodes.

\begin{figure}[h]
	\includegraphics[width=1.0\textwidth]{average_svm/linear_reg}
	\centering
	\caption{Validation accuracy obtained for various values of $C$ for the SVM with a linear kernel trained on the node features and the average neighbouring features. The best validation accuracy is obtain at $C=0.018$, with a validation accuracy of 7.14\%.}
	\label{fig/average_svm_linear_reg}
\end{figure}
\begin{figure}[h]
	\includegraphics[width=1.0\textwidth]{average_svm/rbf_reg}
	\centering
	\caption{Validation accuracy obtained for various values of $C$ and $\gamma$ for the SVM with an RBF kernel trained on the node features and the averaged neighbouring features. The best validation accuracy is obtain at $C5.411$ and $\gamma=2^{-9}$, with a validation accuracy of 71.6\%.}
	\label{fig/average_svm_rbf_reg}
\end{figure}

This is a significant improvement over the accuracy obtain with the SVM trained on the features of each node individually. This shows that incorporating knowledge about the graph structure does indeed improve the classification accuracy significantly even when incorporated only in a very simplistic way, i.e.\ by only considering the direct neighbours and averaging their features.

\subsection{Graph Convolution Network}
For this model I used the hyper-parameters presented by the authors which were obtained by optimising the validation accuracy on the set of 500 validation nodes. The model is trained over 200 epochs using the Adam optimiser \cite{kingma2014adam}. The optimal learning rate was found to be 0.01 The weights are initialised using a Glorot initialization \cite{glorot2010understanding}. Furthermore, the input feature vectors are row-normalized. The dropout rate was set to 0.5, the L2 regularisation weight to 0.0005 and the number of hidden inuts was set to 16. Using my implementation of this algorithm in TensforFlow, and averaging the result of 10 runs, I obtain an accuracy of 78.5\% on the validation data and 80.3\% on the test data.

This is again an improvement over the average MLP which could be explained by different reasons. First, the graph structure is incorporated in a more principled way, using the adjacency matrix and the degree matrix of the network. Second, as the network consists of two layers, it has the ability to incorporate information not only from the direct neighbours of a node but also from the neighbours of the neighbours of a node. The first layer incorporates at each node information about its direct neighbours. Using the output of the first layer, the second layer now has indirect access to the features of the neighbours of the neigbours of a node. Hence, the graph structure is incorporated with a distance of 2 rather than just 1 as was the case when averaging.

\subsubsection{Hidden layer visualisation}
The activations in the hidden layer of the 2 layer GCN can be visualised using a dimensionality reduction technique. Following the approach taken by Kipf and Welling, I use t-SNE \cite{maaten2008visualizing} to reduce the 16-dimensional hidden activation to 2 dimensions. Figure \ref{fig/gcn_tsne} shows the resulting vectors for the training, validation and test datasets.
\begin{figure}[h]
	\includegraphics[width=1.0\textwidth]{gcn/visualisation_tsne}
	\centering
	\caption{CAPTION}
	\label{fig/gcn_tsne}
\end{figure}

\subsection{Graph Attention Network}
As for the GCN, I use the hyper-parameters presented by the author but reimplement the network myself in TensorFlow. The regularisation weight is set to $\lambda = 0.0005$ and the dropout rate is set to $p=0.6$. The Adam optimiser \cite{kingma2014adam} is used again with a learning rate of $\textit{lr} = 0.005$. The model is trained over 100 epochs, each with a single batch containing all 2708 nodes.

Using these hyper-parameters I obtain a validation accuracy of 0.77\% and a test accuracy of 0.79\%. This is below the accuracy reported by the authors and I cannot explain as to why those results are different, except if my implementation is not exactly the same as the one proposed in the paper. 
\begin{figure}[h]
	\includegraphics[width=1.0\textwidth]{gat/visualisation_tsne}
	\centering
	\caption{CAPTION}
	\label{fig/gcn_tsne}
\end{figure}
\section{Conclusion}

\bibliography{bib}
\bibliographystyle{unsrt}


\end{document}